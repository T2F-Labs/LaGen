# Research Guide

---

## ğŸ¯ Task Types and Research Directions

- **Neurosymbolic RL** â€“ Combining symbolic reasoning with reinforcement learning
- **Program Synthesis** â€“ Automated code generation through RL
- **Code-based RL Tasks** â€“ Reinforcement learning in programming environments

---

## ğŸŒ Environments and Benchmarks

### ğŸ“‹ Key Environments

- **BabyAI** â€“ Language-to-action environment for grounded instruction following
- **TextWorld** â€“ Text-based game environment for natural language RL
- **CodeX / CodexArena** â€“ Code-based environments for programming RL research

### ğŸ—ï¸ Environment Structure Categories

**Environment Types:**

- Structured vs Unstructured Environments
- Dynamic vs Static Environments
- Discrete RL Tasks
- Multi-Agent vs Single-Agent Environments

**State Management:**

- **Inventory Management as Sub-State in RL:**
    - Permanent Inventory
    - Consumable Inventory
- **Shared (Global) State vs Local State:**
    - Example: Shared inventory or knowledge between agents

---

## ğŸ Reward Design Principles

### â° Core Reward Concepts

- **Delayed Rewards and Temporal Credit Assignment**
- **Reward = 0 is Acceptable** if Delayed Returns are Used (i.e., Discounted Future Rewards)
- **Neutral â‰  Worthless:** Actions with expected cumulative reward = 0 neither help nor hurt

### âš ï¸ Avoiding Poor Reward Shaping

- Too Many Zero-Reward Actions
- No Penalty for Wasting Time
- Misaligned Reward Design Can Block Skill Discovery
- Context-Aware or Long-Term Reward Shaping
- Use Rewards at the End (Sparse or Delayed Reward)
- Use Curiosity or Intrinsic Motivation
- Time-Based Penalties (e.g., for excessive steps)
- **Reward Delta (Net Progress):** E.g., Reward based on delta-x (distance moved toward goal)

### ğŸ§  Advanced Reward Principles

- Don't punish necessary short-term sacrifices for long-term gain
- Encourage counterintuitive actions when they serve a greater goal:
    - E.g., moving left to build momentum for a rightward jump
- **ğŸ§  Principle: Reward Outcomes, Not Actions** â€“ Reward results instead of just behaviors

---

## ğŸ” Smarter Exploration Techniques

- **Curiosity-Driven Agents** â€“ Reward internal novelty
- **Entropy Regularization** (e.g., in PPO) â€“ Encourage action diversity
- **Count-Based Exploration** â€“ Reward visiting new or rare states

---

## â±ï¸ Temporal Credit Assignment Techniques

- **TD(Î»)** â€“ Temporal difference with eligibility traces
- **Monte Carlo Tree Search** â€“ Planning over sequences

---

## ğŸ¤– Human-like Generalization in RL

Advanced agents aim to incorporate concepts such as **object permanence**, **semantic similarity**, symbolic reasoning, and inductive bias.

### ğŸ—£ï¸ Language-Conditioned RL

- Use language as a symbolic interface
- Example: "Avoid the monsters and reach the flag" â†’ reward shaping + attention bias
- Large Language Models (LLMs) guide DRL agents
- Applied in Minecraft, BabyAI, OpenAI's InstructPix2Seq, and more

### ğŸ¯ Approaches for Human-like Generalization

1. **ğŸŒ World Models** (e.g., Dreamer, MuZero)
    - Agents learn compressed models of the environment
    - Predict consequences of actions for internal simulation and planning
    - Helps generalize transitions, but limited in concept abstraction
2. **ğŸ”— Neuro-symbolic RL**
    - Combine RL with symbolic reasoning or object detection
    - Examples: "That's a monster" â†’ "Avoid"
    - Tools: Causal RL, Object-Centric RL
3. **ğŸ’¬ Language-Guided RL**
    - Integrate language priors or instructions (e.g., GPT-4 + RLHF, VLN agents)
    - Use LLMs for semantic understanding: "What is this object?"
    - Combine vision and language for grounded RL
4. **ğŸ“ Meta-Learning / Few-Shot RL**
    - Learn how to learn, generalize concepts like "monster" not just instances
    - Tools: MAML, PEARL, RLÂ²
5. **ğŸ“š Curriculum Learning**
    - Gradual exposure to harder tasks
    - Example: Easy monsters â†’ harder variations

### ğŸ› ï¸ Frameworks and Tools

- **Object-Centric RL (OCRL)** - [GitHub Repository](https://github.com/jsikyoon/OCRL) | [Project Website](https://sites.google.com/view/ocrl/home)
- **CausalWorld** - [GitHub Repository](https://github.com/rr-learning/CausalWorld)
- **Neurosymbolic Concept Learners** - [Neuro-Symbolic Concept Learner](http://nscl.csail.mit.edu/) | [Awesome Neural-Symbolic Papers](https://github.com/Ying1123/awesome-neural-symbolic)

### ğŸ’» Programmatic Policy Design (DSL + DRL)

- **Neural Module Networks** - [Neurosymbolic Topics on GitHub](https://github.com/topics/neurosymbolic)
- **DreamCoder (Program Induction + RL)** - [Related: LILO Framework](https://arxiv.org/html/2310.19791v3)
- **DSL-guided RL** - [Neurosym Framework](https://neurosymbolic-learning.github.io/README.html)

---

## ğŸ—ï¸ Hierarchical and Modular RL

- **Inverse Reinforcement Learning (IRL):** Learn from demonstrations
- **Hierarchical Reinforcement Learning (HRL):** Learn sub-policies or skills (e.g., "long jump")
- **World Models:** Predict consequences before acting
- **Frameworks:** Option-Critic, FeUdal RL, Meta-Learning

---

## ğŸ›ï¸ Policy Architectures and Methods

- **Memory-Based Agents** (e.g., recurrent networks)
- **PPO, DQN, Transformer-based RL** (via masked logits)
- **Libraries:** RLlib, Stable-Baselines, etc.
- **Conditioned Action Networks:**
    - Multi-head or Gated Policy Branches
- **Learnable Affordances:**
    - Agent learns which actions matter based on emergent behavior

---

## ğŸ›¡ï¸ Safe Masked Policy Flow

### âœ… Best Practice: Double Safety Tips Step

| Step | Description | Purpose |
| --- | --- | --- |
| 1. Mask logits before softmax | `logits[~mask] = -inf` | Ensure invalid actions have zero probability |
| 2. Mask during sampling | Sample only from valid actions | Double-check valid action selection |
| 3. Mask during loss computation | Prevent gradients through invalid actions | Avoid learning invalid behavior |
| 4. Use assertions during debug | `assert probs[invalid_action] == 0` | Verify safety constraints |
| 5. Log and visualize mask | Track what the agent sees | Monitor masking behavior |

---

## ğŸ® Real-Game Action Categories

### ğŸ§¾ Action Types Overview

| **Action Type** | **Description** |
| --- | --- |
| Self-only | Affects only the agent's own state |
| Opponent-only | Affects only others in the environment |
| Shared/Environment | Affects the global state or environment |
| Joint | Requires or causes a coordinated interaction |
| Neutral | Has no observable effect (yet) |

### ğŸ¯ Examples from Games

| **Action** | **Agent Affected** | **Action Type** | **Effect** |
| --- | --- | --- | --- |
| Jump | Agent | Self-only | Change own position |
| Reload | Agent | Self-only | Change own ammo count |
| Shoot | Other agents | Opponent-only / Joint | Damages enemy |
| Block | Agent + Other | Joint | Modifies effect of enemy attack |
| Capture flag | All agents | Shared | Changes global win condition |
| Use medkit | Agent | Self-only | Increase own health |
| Throw grenade | Agent + Others | Shared + Opponent | Affects area and enemies |
| Taunt | Other agents | Opponent-only | Might change their behavior |
| Call airstrike | Shared/Environment | Shared/global | Area-of-effect over environment |

### ğŸ”„ Why This Matters

1. **Learning Dependencies**
    - *Self-only* actions can be trained with individual rewards
    - *Opponent-affecting* actions might need multi-agent coordination or game-theoretic reasoning

---

## ğŸ•’ Time-Aware Environments

**Time-related functionality** means the environment evolves over time, even without agent interaction.

### â° Examples

- â²ï¸ Countdown timers
- ğŸ•¹ï¸ Moving enemies (patrolling)
- ğŸŒ«ï¸ Weather or environmental changes
- â›³ Falling platforms
- ğŸ’£ Bombs with timers

Even with **NO-OP (no action)**, the state may still transition.

### âœ… Formal View (Markov Decision Process)

- State transition governed by: `s_{t+1} ~ P(s_{t+1} | s_t, a_t)`
- In partially autonomous environments: `s_{t+1} â‰  s_t` even if `a_t = NO-OP`

---

## ğŸŒ Real World Complexity: Multi-Dimensional States + Actions

The real world consists of **multi-dimensional states** and **multi-dimensional actions**.

**To handle this situation:**

- **Multi-Discrete Action Spaces** (as used in OpenAI Gym)

---

## ğŸ§  Multiple Simultaneous Actions in One State

### ğŸ¯ Solutions Overview

1. **Multi-Discrete Action Space** â€“ Vector of actions
2. **Multi-Binary Action Space** â€“ On/Off for each action
3. **Factorized Policy Network** â€“ Multiple output heads
4. **Constraint-Aware Action Masking** â€“ Enforce valid action sets
5. **ğŸ§© Combined Multi-Head Policy Design** â€“ Blend specialized policy branches

### ğŸ”„ Action Categories

| **Category** | **Examples** | **Description** |
| --- | --- | --- |
| ğŸŸ¢ **Always-On / Must Use** | Running, movement | Agent must continuously select or maintain |
| ğŸŸ¡ **Conditional / Contextual** | Shooting, reloading | Only used when needed |
| ğŸ”¹ **Advisable / Opportunistic** | Aiming, rotating camera, crouching | Useful but not mandatory |

### ğŸ’¡ Solutions by Category

**ğŸŸ¢ Always-On Actions (Core Controls)**

- These are constantly in effect and require consistent decision-making

**ğŸŸ¡ Contextual Actions (Conditional Fire)**

- âœ… **Solutions:**
    - **Action Masking:** Mask "shoot" if no enemy is visible
    - Mask "reload" if ammo is full

**ğŸ”¹ Opportunistic Actions (Strategic Enhancers)**

- Actions that improve performance but aren't always required

---

## ğŸ§  Multi-Modal State Handling

### ğŸ“Š Components of the State in DRL

| **Component** | **Description** | **Is it part of the State?** |
| --- | --- | --- |
| Ego state | Agent's own internal state (e.g., health, ammo, position, velocity, etc.) | âœ… Yes |
| Opponent state | Observed or inferred state of enemies or other players (e.g., position, health) | âœ… Yes |
| Environment context | Map layout, time left, weather, moving platforms, gravity, etc. | âœ… Yes |
| Latent dynamics | Time-based changes (e.g., timer tick, NPC patrols) even without agent input | âœ… Yes |
| Camera view / screen | What the agent can currently see (partial observability) | ğŸ¯ Part of observable state |

**ğŸ¯ Note:** In Deep Reinforcement Learning (DRL), we typically define the state at time *t* as:

**State sâ‚œ = All observable information at time t**

### ğŸ” How We Handle Complex State Spaces

Here's how researchers and engineers tame the chaos:

1. **State Abstraction**
2. **Feature Engineering or Feature Learning**
3. **State Factorization**
4. **Attention Mechanisms**
5. **Graph Representations** (Neuro-symbolic style)
6. **Hierarchical RL**

### ğŸ§  Agent Group Decision

- Multiple agents' observations (e.g. teammates) â†’ decide one shared team action
- Each state comes from a different entity, but action is centralized
- Common in multi-agent coordination or CTDE (Centralized Training, Decentralized Execution)

### ğŸ—ï¸ Architecture Tips

**Modular Encoders â†’ Fusion**

- Transformer-based Attention Fusion
1. **Separate Encoders per Modality**
    - Each input domain (e.g., image, sound, sensors) gets its own neural module
    
    ```python
    vision_embedding = CNN(image)
    audio_embedding = RNN(audio)
    game_embedding = MLP(game_state)
    fused = concat(vision_embedding, audio_embedding, game_embedding)
    
    ```
    
    - Keeps things clean and modular
2. **Attention or Gating**
    - Let the agent learn which state sources matter most in each scenario
    
    ```python
    contextual = Transformer([vision, audio, game])
    
    ```
    
    - Or gated networks:
    
    ```python
    g = sigmoid(weighted_sum(inputs))
    action = f(g * vision + (1 - g) * audio)
    
    ```
    
3. **Curriculum Learning**
    - Don't train on the full state space from the start
    - Start with vision only â†’ add game logic â†’ add audio
    - Helps stabilize learning
4. **Latent State Compression**
    - Compress raw state components into a latent bottleneck
    
    ```python
    compressed = encoder(state)
    action = policy(compressed)
    
    ```
    

---

## ğŸ“Œ Strategy Summary

| Strategy | Description | Use Case | Difficulty |
| --- | --- | --- | --- |
| Action Masking | Manually block invalid actions | Classic discrete RL | â­â­ |
| Hierarchical RL | Learn skills & when to use them | Multi-stage action sequences | â­â­â­â­ |
| Gated Action Heads | Modular policy branches | Context-specific control | â­â­â­ |
| Learnable Affordances | Learn consequences of key actions | Emergent skills & affordances | â­â­â­â­ |
| World Models | Use environment modeling for planning | Foresight and programmatic env | â­â­â­â­â­ |
| Dynamic Action Space | Action set varies with environment state | Strong agent-environment coupling | â­â­â­â­ |

---

## ğŸ§© Preventing Hallucinated Actions

### ğŸ’¡ Solutions

**âœ… 1. Outcome-Based Rewards**
Focus rewards on **achieved results**, not mere behaviors. Reward the consequences of actions, not just the actions themselves.

**âœ… 2. State-Aware Reward Conditions**
Design rewards that depend on **environment state** or context. For example, only reward "healing" if health is low, or "reloading" if ammo is empty.

**âœ… 3. Action Penalty Budget**
Assign **costs to actions** to discourage spammy or reckless behavior. This introduces **opportunity cost** and encourages strategic planning.

| **Action** | **Penalty** | **Purpose** |
| --- | --- | --- |
| Shooting | â€“0.1 | Discourage overfiring |
| Moving | â€“0.01 | Penalize aimless or excessive motion |

This budgeting strategy promotes **efficiency and intentionality** in action selection.

---

## ğŸ­ Trait-Driven Agents

### ğŸ¤– Traditional RL vs Trait-Driven Agents

**Traditional RL** is reward-driven â€” agents optimize for reward signals but lack **identity**, **style**, or **personality**.

By contrast, **trait-driven agents** exhibit consistent behavioral **biases or tendencies** that transcend short-term reward maximization.

### ğŸ§¬ What Are Traits in Cognitive Agents?

**Traits** represent *meta-policies* â€” consistent, goal-modulating preferences that influence decision-making across tasks.

| **Trait** | **Behavioral Expression** |
| --- | --- |
| Courage | Fights stronger enemies; avoids fleeing |
| Curiosity | Explores unknown zones despite danger |
| Greed | Prioritizes high-reward options even under high risk |
| Empathy | Helps other agents in danger |
| Caution | Avoids danger; prefers safety over speed or gain |
| Discipline | Rejects short-term gain for long-term strategy |

Traits aren't just learned from reward â€” they *shape how rewards are pursued*.

### ğŸ§  How Can Traits Be Implemented?

1. **Policy Conditioning on Trait Vectors**
Inject fixed or learned trait embeddings into the policy network input.
2. **Multi-Objective RL with Trait Weights**
Define reward as a weighted sum of competing goals:
R = wâ‚ Â· survival + wâ‚‚ Â· exploration + wâ‚ƒ Â· combat
3. **Neuro-Symbolic Trait Modulation**
Incorporate rules or logic to reflect trait-driven decisions:
    - *IF* `Trait = Curious` *AND* `Nearby = Unknown` â†’ **Explore**
    - *IF* `Trait = Empathetic` *AND* `Other Agent = In Danger` â†’ **Help**
4. **Meta-RL or Trait Learning**
Learn traits as latent variables or modulators through meta-learning.

### ğŸŒŸ Why Traits Matter for Real Intelligence

Traits enable:

- âœ… **Individualism** in multi-agent systems (diverse behaviors, unique personalities)
- âœ… **Alignment** with human values (e.g., humility, caution, cooperation)
- âœ… **Transferability** across tasks â€” *traits generalize where rewards do not*
- âœ… **Narrative consistency** in agents (e.g., NPCs behaving believably in story contexts)

### ğŸ”® Applications and Future Directions

- ğŸ® **Game AI:** Characters with meaningful and coherent personalities
- ğŸ¤– **Social Robots:** Emotionally and morally aligned behavior
- ğŸ›°ï¸ **Autonomous Agents:** Consistent decision-making aligned with identity
- ğŸ™ï¸ **Simulated Societies:** Diverse agents with individual priorities and worldviews

---

## ğŸ§  Trait Expression Mechanisms

### ğŸ¯ Expression Types

Agents can exhibit **traits** either through **explicit conditioning** or through **emergent behavior** shaped by experience or selection.

### 1. ğŸ§¬ Trait-Conditioned Learning (Trainable Personality)

Rather than hard-coding behaviors, the agent learns to adapt its policy based on an input **trait vector**.

Ï€(a|s,Ï„)

Where:

- s = state
- Ï„ = trait vector (e.g., courage, caution, greed)

**Example:**
Ï„ = [courage=0.9, caution=0.2, greed=0.5]

This allows for **dynamic personality control** and **interpersonal diversity** across agents.

### 2. ğŸŒ± Emergent Traits (Unsupervised or Evolved)

Traits can arise *organically*, without being explicitly defined, via mechanisms like:

- **Evolutionary Algorithms**
    - A population of agents evolves diverse strategies
    - e.g., some become brave, others cautious, based on fitness dynamics
- **Meta-RL with Episodic Memory**
    - Agents adapt their strategy across episodes
    - Context-dependent behaviors emerge (e.g., "avoidant" vs "aggressive")
- **Intrinsic Motivation Signals**
    - Curiosity, novelty-seeking, or empowerment-based rewards
    - Agents behave *as if* they have traits like curiosity or ambition

Emergent traits reflect **natural diversity** and can support **robust, adaptive behavior** in complex environments.

---

## ğŸ” Related Research Areas

### ğŸ§  Is There a "Traits-Based RL" Field?

There is **no formally defined subfield** called *Traits-Based Reinforcement Learning* â€” but related methods and architectures exist under various names.

### ğŸ”¬ Related Research & Engineering Paradigms

| **Area** | **Description** |
| --- | --- |
| ğŸ§  Goal-Conditioned RL (GCRL) | Policies adapt based on goals, often interchangeable with traits |
| ğŸ­ Persona-Conditioned RL | Explicit personality modeling (e.g., dialogue, NPC behavior) |
| ğŸŒ€ Meta-RL / RLÂ² | Agents learn to generalize behaviors across tasks (traits emerge) |
| ğŸ§¬ Evolutionary & Population RL | Traits evolve over generations of agents |
| ğŸ¤– Human-Like or Social RL | Alignment with human traits and social context |
| ğŸ§© Modular / Neuro-Symbolic RL | Trait modules influence symbolic reasoning or learned policies |
| ğŸ§ Cognitive Architectures | ACT-R, SOAR, AGI-inspired planners embedding human-like behavior |

---

## ğŸ§± Core Engineering Directions for Trait-Centric RL

### 1. ğŸ¯ Goal- or Trait-Conditioned Policies

Policies are conditioned on a **latent vector** encoding personality or goal-like traits:

Ï€(a|s,z), where z = trait or goal embedding

Example:
Inject `z = [courage=0.9, curiosity=0.5, patience=0.1]` into the policy.

**Examples in Practice:**

- Google's **Universal Policies**
- OpenAI's **meta-learned agents** with embedded task context

### 2. ğŸ§¬ Population-Based or Evolutionary RL

A **diverse agent population** evolves over time, each with a genome encoding behavioral tendencies.

- Traits like *aggressiveness*, *cooperation*, or *greed* emerge through selection
- Effective for survival games, team simulations, or open-ended tasks

**Tools:**
NEAT, ES-HyperNEAT, POET, Population-Based Training (PBT)

### 3. ğŸŒ€ Meta-RL / RLÂ²

**Meta-learning** allows agents to learn how to learn â€” traits emerge **episodically**, not explicitly.

- In one task: the agent behaves cautiously
- In another: it's aggressive or risk-seeking
- Traits are **adaptive** and **context-dependent**

### 4. ğŸ­ Persona-Based RL (Especially for Dialogue Agents)

Used in **language models**, **games**, and **simulations**.

- Agents behave according to a defined persona
- Trait vectors guide speech, decision-making, and emotional tone

**Examples:**
Voyager, AI Dungeon, Inworld AI â€” combine **LLMs + RL** for character-rich behaviors

### 5. ğŸ§© Neuro-Symbolic or Modular Trait Controllers

Traits are implemented as **symbols, logic, or modular controllers**, shaping actions or reward pathways.

Examples:

- **Courage** â†’ lowers threshold for risky actions
- **Curiosity** â†’ increases novelty-based intrinsic reward
- **Empathy** â†’ modifies cooperative or moral decisions

**Frameworks:**
SOAR, ACT-R, MindsEye, LEGO-style Modular RL

### 6. ğŸ”§ Trait-Extensible RL Frameworks

| **Framework** | **Trait Support** | **Notes** |
| --- | --- | --- |
| Stable-Baselines3 | âœ…âœ… (via wrappers) | Add trait-conditioning wrappers easily |
| RLlib (Ray) | âœ…âœ… | Built-in multi-agent and curriculum learning support |
| PettingZoo | âœ… | Multi-agent simulations; great for trait diversity |
| CleanRL | âš ï¸ | Minimalist; harder to extend for traits |
| OpenSpiel | âœ… | Turn-based, supports agents with roles or personality |