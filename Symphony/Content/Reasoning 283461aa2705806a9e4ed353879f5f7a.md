# Reasoning

## The Chicken-Egg Situation

We are in a chicken-egg situation because:

- The Conductor (Python) needs to use the Rust IPC bus to communicate with out-of-process extensions.
- The Rust IPC bus is part of the Orchestra Kit, which is managed by the Conductor.

But note:

- The Rust IPC bus is a hardcoded Rust crate.
- The Conductor (Python) will use it via FFI (PyO3).
- So the Conductor must load the Rust IPC bus (as a shared library) at startup.

However, the Rust IPC bus itself is part of the application and is available at build time.

## Breaking Down the Bootstrapping

1. The application starts with the Conductor (Python) as the main entry point.
2. The Conductor, during its initialization, loads the Rust IPC bus shared library (via PyO3) and initializes it.
3. The Rust IPC bus then sets up the necessary infrastructure (like the IPC socket) and waits for connections.

But wait, the Conductor also needs to load the in-process extensions (The Pit) which are also written in Rust. How are they loaded?

### Two Kinds of Rust Components

**a) The Rust IPC bus**

- A shared library loaded by Python via FFI

**b) The Pit extensions**

- Rust libraries that are loaded in-process by the Rust IPC bus? Or by the Conductor? Or by the Orchestra Kit?

### Clarification

The Pit extensions are in-process, meaning they are loaded into the same process as the Conductor (Python). But the Conductor is a Python process, so how can it load Rust libraries? It must do so via the same FFI (PyO3) mechanism.

So, the Conductor (Python) will load:

- The Rust IPC bus (as a shared library) via PyO3.
- The Pit extensions (as Rust shared libraries) via the same PyO3, but note that the Pit extensions are separate from the IPC bus.

But wait, the Rust IPC bus and the Pit extensions are both Rust code.

## Structural Options Considered

### Option 1: Combined Library

We can structure the code as:

**symphony_ipc_bus**: A Rust library that provides the IPC bus functionality and also exposes the Pit extensions (or at least a way to load them).

Alternatively:

**symphony_core**: A Rust library that is loaded by the Conductor (Python) via PyO3. This Rust library includes:

- The IPC bus functionality
- The Pit extensions (as separate modules, but compiled into the same shared library)

But note: The Pit extensions are supposed to be isolated and replaceable. If we compile them into the same shared library, then they are not replaceable without recompiling.

### Option 2: Dynamic Loading

Alternatively, we can have:

- The Conductor (Python) loads the Rust IPC bus (shared library A) and then the Rust IPC bus loads the Pit extensions (shared libraries B, C, D, ...) as dynamic libraries.

But then we have two levels of dynamic loading:

- Python loads Rust (via PyO3) for the IPC bus and the Pit extensions.
- The Rust IPC bus (which is in the same process) loads the Pit extensions (also in the same process) as dynamic libraries.

But note: The Pit extensions are in-process, so they are in the same process as the Conductor (Python) and the Rust IPC bus.

### Option 3: Static Linking

Alternatively, we can structure the code as:

- The Conductor (Python) loads a single Rust shared library (via PyO3) that contains:
    - The IPC bus code
    - The code for all the Pit extensions (statically linked)

But then we lose the modularity of the Pit extensions.

### Chosen Approach: Dynamic Loading by IPC Bus

Alternatively, we can have:

- The Conductor (Python) loads the Rust IPC bus (shared library A) and then the Rust IPC bus loads the Pit extensions as dynamic libraries (shared libraries B, C, D, ...).

This is feasible and maintains modularity.

But note: The Pit extensions are infrastructure and are trusted, so we don't need the same level of isolation as user extensions. However, we still want to be able to update one without replacing the entire binary.

So, let's go with:

- The Conductor (Python) loads the Rust IPC bus (as a shared library) via PyO3.
- The Rust IPC bus, upon initialization, loads the Pit extensions (which are separate shared libraries) from a known location (e.g., the installation directory).

## Bootstrapping Sequence

1. The Conductor (Python) starts.
2. The Conductor loads the Rust IPC bus shared library (via PyO3) and calls an initialization function.
3. The Rust IPC bus initialization function:
    - a. Sets up the IPC socket for out-of-process extensions.
    - b. Loads the Pit extensions (the in-process ones) from the file system (e.g., libpool_manager.so, libdag_tracker.so, etc.) and initializes them.
4. The Conductor then uses the Rust IPC bus to start the out-of-process extensions (if any) and to communicate with both the Pit (in-process) and user extensions (out-of-process).

## Calling Pit Extensions

But note: The Conductor must be able to call the Pit extensions directly (because they are in-process and performance-critical).

How does the Conductor call the Pit extensions?

### Option A: Via the Rust IPC Bus

The Conductor calls the Pit extensions via the Rust IPC bus. But then we are adding an extra hop (from Python to Rust IPC bus to Pit extension) which is not necessary because they are in the same process.

### Option B: Direct Handles

The Conductor gets direct handles to the Pit extensions (via the Rust IPC bus) and then calls them directly (via PyO3). But then the Rust IPC bus must expose the Pit extensions to the Conductor.

Let's consider Option B:

- The Rust IPC bus, when loading a Pit extension, returns a handle (a function pointer or a struct of function pointers) to the Conductor (Python) for that extension.
- The Conductor then can call the Pit extension directly (via PyO3) without going through the IPC bus.

But note: The Pit extensions are Rust code, so the Conductor (Python) must use the PyO3 interface to call them. This means that each Pit extension must have a PyO3 wrapper.

Alternatively, we can have the Rust IPC bus act as a proxy for the Pit extensions. The Conductor calls the Rust IPC bus (which it already has a PyO3 interface for) and the Rust IPC bus then calls the Pit extension. This is Option A.

The performance of Option A might be acceptable because it's still in-process (just an extra function call). But we are designing for microsecond-level performance, so we want to avoid extra hops.

### Performance Comparison

**Option A (proxy through IPC bus):**

```
Python -> PyO3 -> Rust IPC bus -> Pit extension

```

**Option B (direct):**

```
Python -> PyO3 -> Pit extension

```

Option B is one less hop.

### Achieving Option B

So how can we achieve Option B?

We can have the Rust IPC bus, when loading a Pit extension, also create a PyO3 wrapper for that extension and register it with the Python interpreter. Then the Conductor (Python) can import and use the Pit extension directly.

But note: The Conductor doesn't know about the Pit extensions at compile time. They are dynamically loaded. So we cannot have pre-generated PyO3 bindings for each Pit extension.

Alternatively, we can have a generic PyO3 interface for Pit extensions. Each Pit extension implements a common trait (e.g., PitExtension) that has a standard set of methods (like handle_message). Then the Rust IPC bus can load the Pit extension and register it with the Conductor as a generic extension. The Conductor then uses the same generic interface to call any Pit extension.

But then we lose the type-specific methods of each Pit extension.

Alternatively, we can generate PyO3 bindings for each Pit extension at build time and then the Conductor can use them directly. But then we are back to having the Conductor know about the Pit extensions at compile time, which we don't want.

Alternatively, we can have the Rust IPC bus expose a generic call method for each Pit extension, and then the Conductor uses that generic method to call into the Pit extension. This is similar to Option A, but we are going to do it for both in-process and out-of-process extensions.

## Unified Interface Solution

Wait, we already have a generic call method for extensions (the send_message method). We can use that for both in-process and out-of-process extensions.

But then we are treating the Pit extensions (in-process) the same as user extensions (out-of-process) from the Conductor's perspective. The Conductor doesn't know whether an extension is in-process or out-of-process, it just sends a message and gets a response.

This is the beauty of the Chambering flow and the Orchestra Kit: the Conductor doesn't care about the execution model.

### Final Approach

So, let's go with:

- The Conductor (Python) uses the Rust IPC bus to send messages to extensions (both in-process and out-of-process).
- For in-process extensions (The Pit), the Rust IPC bus will route the message to the appropriate Pit extension (which is loaded in the same process) and return the response.
- For out-of-process extensions, the Rust IPC bus will route the message via IPC to the extension process and then return the response.

This way, the Conductor has a uniform interface for all extensions.

## Final Bootstrapping Sequence

1. Conductor (Python) starts.
2. Conductor loads the Rust IPC bus (shared library) via PyO3 and initializes it.
3. The Rust IPC bus, during initialization, loads the Pit extensions (as dynamic libraries) and sets up the IPC socket.
4. The Conductor then uses the Rust IPC bus to send messages to extensions (by extension ID) and the Rust IPC bus routes them appropriately.

This avoids the chicken-egg problem because the Conductor only depends on the Rust IPC bus, and the Rust IPC bus takes care of loading the Pit extensions and managing the out-of-process extensions.

## Loading Pit Extensions

Now, how does the Rust IPC bus load the Pit extensions?

We can have a configuration (or a fixed directory) from which the Rust IPC bus loads the Pit extensions. The Pit extensions are shared libraries that implement a known trait (e.g., PitExtension). The Rust IPC bus uses libloading or similar to load these libraries and then registers them in an internal map.

So, the Rust IPC bus must be built with the ability to dynamically load the Pit extensions.

But note: The Pit extensions are trusted and are part of the core installation, so we can assume they are available in a specific directory.

## Code Implementation

Now, let's write the bootstrapping code in Python and Rust:

### Python Side (Conductor)

```python
# conductor.py
import symphony_ipc  # This is the PyO3 module for the Rust IPC bus

class Conductor:
    def __init__(self):
        # Load the Rust IPC bus and initialize it
        self.ipc_bus = symphony_ipc.IPCBus()
        self.ipc_bus.initialize()

    async def start(self):
        # Start the IPC bus (which loads the Pit extensions and starts the IPC server)
        await self.ipc_bus.start()

    async def call_extension(self, extension_id, message):
        return await self.ipc_bus.send_message(extension_id, message)

```

### Rust Side (symphony_ipc_bus)

```rust
// symphony_ipc_bus/src/lib.rs
use libloading::{Library, Symbol};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::Mutex;

type PitExtension = dyn Fn(serde_json::Value) -> serde_json::Value + Send + Sync;

pub struct IPCBus {
    pit_extensions: HashMap<String, Arc<PitExtension>>,
    // ... other fields for out-of-process extensions
}

impl IPCBus {
    pub fn new() -> Self {
        Self {
            pit_extensions: HashMap::new(),
        }
    }

    pub fn initialize(&mut self) {
        // Load Pit extensions from a directory
        let pit_extension_paths = std::fs::read_dir("/path/to/pit/extensions").unwrap();

        for path in pit_extension_paths {
            let path = path.unwrap().path();
            if path.extension().and_then(|s| s.to_str()) == Some("so") {
                self.load_pit_extension(&path).unwrap();
            }
        }
    }

    fn load_pit_extension(&mut self, path: &std::path::Path) -> Result<(), Box<dyn std::error::Error>> {
        unsafe {
            let lib = Library::new(path)?;
            let init: Symbol<unsafe extern "C" fn() -> *mut ()> = lib.get(b"init")?;
            let extension_ptr = init();
            let extension: Arc<PitExtension> = Arc::from_raw(extension_ptr as *const PitExtension);
            let extension_id = path.file_stem().unwrap().to_str().unwrap().to_string();
            self.pit_extensions.insert(extension_id, extension);

            // Note: We are leaking the library because we are not storing it. We need to keep the library loaded.
            // We can store the Library in a vector to prevent unloading.
            // Alternatively, we can require that the Pit extensions are statically linked.
        }
        Ok(())
    }

    pub async fn send_message(&self, extension_id: String, message: serde_json::Value) -> serde_json::Value {
        if let Some(extension) = self.pit_extensions.get(&extension_id) {
            // This is a Pit extension (in-process)
            extension(message)
        } else {
            // This is an out-of-process extension, send via IPC
            // ...
        }
    }
}

```

But note: The above code for loading Pit extensions is unsafe and requires that the Pit extensions expose a C ABI function init that returns a function pointer.

We can define a trait for Pit extensions and then use a macro to generate the init function.

## Simplified Approach: Static Linking

Alternatively, we can statically link the Pit extensions into the Rust IPC bus. This would simplify the loading and avoid libloading.

Since the Pit extensions are core and trusted, we can do:

```rust
// In symphony_ipc_bus/src/lib.rs
use pool_manager::PoolManager;
use dag_tracker::DagTracker;
// ... other Pit extensions

impl IPCBus {
    pub fn new() -> Self {
        let mut pit_extensions = HashMap::new();

        pit_extensions.insert("pool_manager".to_string(), Arc::new(PoolManager::new()) as Arc<PitExtension>);
        pit_extensions.insert("dag_tracker".to_string(), Arc::new(DagTracker::new()) as Arc<PitExtension>);
        // ...

        Self { pit_extensions }
    }
}

```

But then we lose the dynamic loading. However, since the Pit extensions are part of the core and we don't expect to update them independently, static linking might be acceptable.

Alternatively, we can use a plugin system that allows both static and dynamic loading, but for now, let's go with static linking for the Pit extensions.

So, the Rust IPC bus will have the Pit extensions compiled into it. Then the Conductor (Python) only needs to load one shared library (the Rust IPC bus) and it will have all the Pit extensions. This avoids the complexity of dynamic loading and the unsafe code.

## Final Bootstrapping Sequence (Static Linking)

1. Conductor (Python) starts.
2. Conductor loads the Rust IPC bus (shared library) via PyO3 and initializes it.
3. The Rust IPC bus, when created, initializes all the Pit extensions (statically linked) and sets up the IPC socket for out-of-process extensions.
4. The Conductor then uses the Rust IPC bus to send messages to any extension (in-process or out-of-process).

This is clean and simple.

## Summary

To summarize:

- The Rust IPC bus is a shared library that is loaded by the Conductor (Python) via PyO3.
- The Rust IPC bus contains the Pit extensions (statically linked) and the IPC server for out-of-process extensions.
- The Conductor uses the Rust IPC bus to communicate with all extensions.

This resolves the chicken-egg problem.

## Final Implementation

Now, let's write the code accordingly.

### Rust IPC Bus (Statically Linked with Pit Extensions)

```rust
// symphony_ipc_bus/src/lib.rs
use pyo3::prelude::*;
use serde_json::Value;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::Mutex;

// Define the trait for Pit extensions
pub trait PitExtension: Send + Sync {
    fn handle_message(&self, message: Value) -> Value;
}

// Implement the trait for each Pit extension
// For example, for PoolManager
pub struct PoolManager;

impl PitExtension for PoolManager {
    fn handle_message(&self, message: Value) -> Value {
        // ... handle the message
        Value::Null
    }
}

// Similarly for other Pit extensions

// The IPCBus struct
pub struct IPCBus {
    pit_extensions: HashMap<String, Arc<dyn PitExtension>>,
}

impl IPCBus {
    #[pyo3(name = "new")]
    pub fn new() -> Self {
        let mut pit_extensions = HashMap::new();

        pit_extensions.insert("pool_manager".to_string(), Arc::new(PoolManager) as Arc<dyn PitExtension>);
        // ... insert other Pit extensions

        Self { pit_extensions }
    }

    #[pyo3(name = "initialize")]
    pub fn initialize(&mut self) {
        // ... any initialization code for the IPC server
    }

    #[pyo3(name = "send_message")]
    pub async fn send_message(&self, extension_id: String, message: Value) -> Value {
        if let Some(extension) = self.pit_extensions.get(&extension_id) {
            extension.handle_message(message)
        } else {
            // ... handle out-of-process extension
            Value::Null
        }
    }
}

// PyO3 module definition
#[pymodule]
fn symphony_ipc(_py: Python, m: &PyModule) -> PyResult<()> {
    m.add_class::<IPCBus>()?;
    Ok(())
}

```

### Python Side (Final)

```python
import symphony_ipc

class Conductor:
    def __init__(self):
        self.ipc_bus = symphony_ipc.IPCBus()
        self.ipc_bus.initialize()

    async def call_extension(self, extension_id, message):
        return await self.ipc_bus.send_message(extension_id, message)

```

This should resolve the bootstrapping issue. Now, the Conductor (Python) can use the Rust IPC bus to call both in-process (Pit) and out-of-process extensions. We have a clear bootstrapping sequence and no chicken-egg problem.